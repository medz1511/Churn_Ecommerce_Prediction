from airflow.decorators import dag, task
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import joblib
import os
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

# --- CONFIGURATION ---
RAW_DATA_PATH = '/opt/airflow/data/raw/data.csv'
PROCESSED_DATA_PATH = '/opt/airflow/data/processed/rfm_churn.csv'
MODEL_PATH = '/opt/airflow/models/model_churn.joblib'
SCALER_PATH = '/opt/airflow/models/scaler_churn.joblib'

# DATE DE COUPURE (3 mois avant la fin du dataset)
# On utilise les donnÃ©es avant cette date pour prÃ©dire ce qui se passe aprÃ¨s
CUTOFF_DATE = '2011-09-01'

default_args = {
    'owner': 'DataScientist',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

@dag(
    dag_id='ecommerce_churn_prediction_pro',
    default_args=default_args,
    start_date=datetime(2023, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    tags=['churn', 'advanced_ml']
)
def churn_pipeline():

    @task()
    def ingest_and_clean():
        print(f"ðŸ“¥ Lecture du fichier : {RAW_DATA_PATH}")
        df = pd.read_csv(RAW_DATA_PATH, encoding="ISO-8859-1")
        
        # Nettoyage
        df = df.dropna(subset=['CustomerID'])
        df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]
        df['TotalAmount'] = df['Quantity'] * df['UnitPrice']
        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
        
        # Sauvegarde temporaire
        clean_path = '/opt/airflow/data/processed/clean_transactions.csv'
        df.to_csv(clean_path, index=False)
        return clean_path

    @task()
    def feature_engineering(input_path: str):
        """
        Logique AvancÃ©e : Temporal Split
        On apprend sur le passÃ© (Observation) pour prÃ©dire le futur (Target)
        """
        print("ðŸ”„ CrÃ©ation des features avec coupure temporelle...")
        df = pd.read_csv(input_path)
        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
        
        cutoff = pd.to_datetime(CUTOFF_DATE)
        print(f"ðŸ“… Date de coupure : {cutoff}")

        # 1. SÃ©paration Observation (PassÃ©) / Target (Futur)
        observation_data = df[df['InvoiceDate'] < cutoff]
        future_data = df[df['InvoiceDate'] >= cutoff]
        
        print(f"   Transactions PassÃ©es : {len(observation_data)}")
        print(f"   Transactions Futures : {len(future_data)}")

        # 2. CrÃ©ation des Features sur le PASSE uniquement
        # La 'Recency' est calculÃ©e par rapport Ã  la date de coupure (le 'prÃ©sent' au moment de l'entraÃ®nement)
        rfm = observation_data.groupby(['CustomerID']).agg({
            'InvoiceDate': lambda x: (cutoff - x.max()).days, # Jours entre dernier achat et la coupure
            'InvoiceNo': 'count',
            'TotalAmount': 'sum'
        })
        rfm.rename(columns={'InvoiceDate': 'Recency', 'InvoiceNo': 'Frequency', 'TotalAmount': 'Monetary'}, inplace=True)

        # 3. CrÃ©ation de la Target (La vÃ©ritÃ© terrain)
        # Quels clients du passÃ© ont achetÃ© dans le futur ?
        customers_who_returned = future_data['CustomerID'].unique()
        
        # Si le client est dans 'future_data', Is_Churn = 0. Sinon Is_Churn = 1
        rfm['Is_Churn'] = rfm.index.isin(customers_who_returned).astype(int)
        # On inverse la logique : isin = True (RestÃ©) -> Churn = 0
        # Donc isin = False (Pas revenu) -> Churn = 1
        rfm['Is_Churn'] = np.where(rfm['Is_Churn'] == 1, 0, 1)

        print(f"ðŸ“Š Distribution Churn RÃ©elle :\n{rfm['Is_Churn'].value_counts(normalize=True)}")
        
        rfm.to_csv(PROCESSED_DATA_PATH, index=False)
        return PROCESSED_DATA_PATH

    @task()
    def train_model(input_path: str):
        print("ðŸ¤– EntraÃ®nement du modÃ¨le PrÃ©dictif...")
        df = pd.read_csv(input_path)
        
        X = df[['Recency', 'Frequency', 'Monetary']]
        y = df['Is_Churn']
        
        # Train/Test Split classique pour valider le modÃ¨le
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        
        # On utilise RandomForest car il gÃ¨re bien les interactions complexes
        model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
        model.fit(X_train_scaled, y_train)
        
        acc = accuracy_score(y_test, model.predict(scaler.transform(X_test)))
        print(f"âœ… Accuracy (Vraie PrÃ©diction) : {acc:.2%}")
        
        joblib.dump(model, MODEL_PATH)
        joblib.dump(scaler, SCALER_PATH)

    path_clean = ingest_and_clean()
    path_rfm = feature_engineering(path_clean)
    train_model(path_rfm)

pipeline = churn_pipeline()